{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fe63d80",
   "metadata": {},
   "source": [
    "# Graph Build + Routing Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c028321a",
   "metadata": {},
   "source": [
    "### Import neccessary libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33cbdbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "from shapely.geometry import LineString, Point\n",
    "from collections import defaultdict\n",
    "\n",
    "import random\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Iterable, List, Optional, Sequence, Tuple, Union\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d162500e",
   "metadata": {},
   "source": [
    "## Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcc22668",
   "metadata": {},
   "outputs": [],
   "source": [
    "junction_df = gpd.read_parquet(\"../data/merged/berlin_bike_accident_node_panel.parquet\")\n",
    "acc_node_df = gpd.read_parquet(\"../data/merged/acc_node.parquet\")\n",
    "\n",
    "# Ensure expected CRS for folium (lat/lon)\n",
    "junction_df = junction_df.to_crs(epsg=4326)\n",
    "acc_node_df = acc_node_df.to_crs(epsg=4326)\n",
    "\n",
    "# If these columns aren't present yet, create them from geometry\n",
    "if \"longitude\" not in junction_df.columns:\n",
    "    junction_df[\"longitude\"] = junction_df.geometry.x\n",
    "if \"latitude\" not in junction_df.columns:\n",
    "    junction_df[\"latitude\"] = junction_df.geometry.y\n",
    "\n",
    "if \"longitude\" not in acc_node_df.columns:\n",
    "    acc_node_df[\"longitude\"] = acc_node_df.geometry.x\n",
    "if \"latitude\" not in acc_node_df.columns:\n",
    "    acc_node_df[\"latitude\"] = acc_node_df.geometry.y\n",
    "\n",
    "# Backwards compatibility: if you didn't add has_crossing, derive it\n",
    "if \"has_crossing\" not in acc_node_df.columns:\n",
    "    acc_node_df[\"has_crossing\"] = acc_node_df[\"node_id\"].notna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78770082",
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_df = gpd.read_parquet(\"../data/merged/berlin_bike_accident_strava_risk_core_panel.parquet\").to_crs(epsg=4326)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025c6b57",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d7398e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crossings_gdf: (2924, 2)\n"
     ]
    }
   ],
   "source": [
    "# one geometry per node_id\n",
    "crossings_gdf = (\n",
    "    junction_df[[\"node_id\", \"geometry\"]]\n",
    "    .dropna(subset=[\"node_id\", \"geometry\"])\n",
    "    .drop_duplicates(subset=[\"node_id\"])\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "crossings_gdf = gpd.GeoDataFrame(\n",
    "    crossings_gdf,\n",
    "    geometry=\"geometry\",\n",
    "    crs=junction_df.crs,\n",
    ")\n",
    "\n",
    "print(\"crossings_gdf:\", crossings_gdf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee595225",
   "metadata": {},
   "outputs": [],
   "source": [
    "rep = segments_df.geometry.representative_point()\n",
    "segments_df[\"longitude\"] = rep.x\n",
    "segments_df[\"latitude\"] = rep.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1935bf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_to_segments = defaultdict(set)\n",
    "\n",
    "for name, geom in zip(segments_df[\"counter_name\"], segments_df.geometry):\n",
    "    for lon, lat in geom.coords:\n",
    "        coords_to_segments[(lat, lon)].add(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f99c859",
   "metadata": {},
   "source": [
    "# Graph + routing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2724b7",
   "metadata": {},
   "source": [
    "Below is a **clean, paper- and repo-friendly Markdown description** you can drop into a README, notebook markdown cell, or a `docs/` page. It’s concise, structured, and matches exactly what the code does—no implementation noise, but no hand-waving either.\n",
    "\n",
    "---\n",
    "\n",
    "## Risk-aware routing graph: overview and structure\n",
    "\n",
    "This module constructs a **month-specific routing graph** from street segments, attaches **exposure-normalized crash risk** at both the segment and junction level, and provides utilities for **shortest-path** and **safety-aware routing**.\n",
    "\n",
    "At a high level, the pipeline is:\n",
    "\n",
    "> **Monthly risk panels → routing graph → edge costs → routing queries**\n",
    "\n",
    "---\n",
    "\n",
    "## Helpers\n",
    "\n",
    "### Node representation\n",
    "\n",
    "Graph nodes are represented as `(x, y)` coordinate tuples in a **metric CRS** (UTM). Nodes correspond to **segment endpoints**, not pre-defined network IDs.\n",
    "\n",
    "### Stable endpoint extraction\n",
    "\n",
    "`_edge_endpoints_as_node_keys` converts a segment LineString into two node keys by:\n",
    "\n",
    "* taking the first and last coordinates,\n",
    "* rounding them (typically to 1 m) to ensure adjacent segments share identical nodes.\n",
    "\n",
    "This makes the graph robust to floating-point and digitization noise.\n",
    "\n",
    "### Risk fallback handling\n",
    "\n",
    "`_risk_fallback_value` defines how missing (`NaN`) risk values are handled:\n",
    "\n",
    "* default: **median** of observed risks in the same month,\n",
    "* alternatives: mean, high (90th percentile), or zero.\n",
    "\n",
    "This avoids treating missing risk as “safe” while keeping routing numerically stable.\n",
    "\n",
    "### Junction risk lookup\n",
    "\n",
    "`_build_node_risk_lookup` constructs a dictionary\n",
    "`node_id → junction risk`\n",
    "for a given `(year, month)` from the junction panel. It:\n",
    "\n",
    "* filters to the month,\n",
    "* ensures one value per junction,\n",
    "* fills missing risks using the fallback policy.\n",
    "\n",
    "### Mapping junctions to graph nodes\n",
    "\n",
    "`_attach_node_ids_to_graph_nodes` spatially **snaps junction points** to the nearest graph node (within a tolerance).\n",
    "This step attaches `node_id` metadata to graph nodes and is **required** for junction risk to influence routing.\n",
    "\n",
    "### Unified edge iteration\n",
    "\n",
    "`_iter_edges` and `_get_edge_data_for_step` abstract over `Graph` vs `MultiGraph` so downstream code can:\n",
    "\n",
    "* iterate edges uniformly,\n",
    "* select the correct parallel edge when summarizing routes.\n",
    "\n",
    "---\n",
    "\n",
    "## Graph construction\n",
    "\n",
    "### Configuration\n",
    "\n",
    "`GraphBuildConfig` centralizes all graph-building decisions:\n",
    "\n",
    "* CRS and endpoint rounding,\n",
    "* exposure and risk column selection,\n",
    "* handling of zero-exposure segments,\n",
    "* fallback policy for missing risk,\n",
    "* whether to allow parallel edges.\n",
    "\n",
    "This makes the graph construction reproducible and schema-robust.\n",
    "\n",
    "### Monthly graph build\n",
    "\n",
    "`build_routing_graph_for_month` constructs a routing graph for a specific `(year, month)`:\n",
    "\n",
    "1. Filter segment panel to the month.\n",
    "2. Project to metric CRS for correct distances.\n",
    "3. Identify exposure and\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26964d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code builds a routing graph for a given (year, month), attaches risk,\n",
    "# and provides shortest / constrained-min-risk routing utilities.\n",
    "\n",
    "\n",
    "# Helpers\n",
    "\n",
    "NodeKey = Tuple[float, float]\n",
    "\n",
    "\n",
    "def _edge_endpoints_as_node_keys(linestring: LineString, ndigits: int = 0) -> Tuple[NodeKey, NodeKey]:\n",
    "    \"\"\"\n",
    "    Create stable node keys from LineString endpoints in a metric CRS.\n",
    "\n",
    "    For UTM meters:\n",
    "      ndigits=0 => 1m rounding (robust)\n",
    "      ndigits=1 => 0.1m rounding (less robust)\n",
    "      ndigits=3 => 1mm rounding (usually too strict)\n",
    "    \"\"\"\n",
    "    (x1, y1) = linestring.coords[0]\n",
    "    (x2, y2) = linestring.coords[-1]\n",
    "    a = (round(float(x1), ndigits), round(float(y1), ndigits))\n",
    "    b = (round(float(x2), ndigits), round(float(y2), ndigits))\n",
    "    return a, b\n",
    "\n",
    "\n",
    "def _risk_fallback_value(series: pd.Series, *, strategy: str = \"median\", default: float = 0.0) -> float:\n",
    "    \"\"\"\n",
    "    Decide a safe fallback risk if a segment/node risk is NaN.\n",
    "\n",
    "    strategy:\n",
    "      - \"median\": median of observed values for that month panel\n",
    "      - \"mean\": mean of observed values\n",
    "      - \"zero\": always 0\n",
    "      - \"high\": conservative high value (90th percentile if possible)\n",
    "    \"\"\"\n",
    "    s = pd.to_numeric(series, errors=\"coerce\")\n",
    "    s = s[np.isfinite(s)]\n",
    "    if len(s) == 0:\n",
    "        return float(default)\n",
    "\n",
    "    if strategy == \"median\":\n",
    "        return float(np.nanmedian(s))\n",
    "    if strategy == \"mean\":\n",
    "        return float(np.nanmean(s))\n",
    "    if strategy == \"high\":\n",
    "        return float(np.nanpercentile(s, 90))\n",
    "    if strategy == \"zero\":\n",
    "        return 0.0\n",
    "\n",
    "    return float(np.nanmedian(s))\n",
    "\n",
    "\n",
    "def _build_node_risk_lookup(\n",
    "    junction_panel_gdf: gpd.GeoDataFrame,\n",
    "    year: int,\n",
    "    month: int,\n",
    "    *,\n",
    "    node_id_col: str = \"node_id\",\n",
    "    node_risk_col: str = \"risk_accidents_per_10k_trips\",\n",
    "    fallback: float = 0.0,\n",
    "    fallback_strategy: str = \"median\",\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Returns dict[node_id(str)] -> node_risk for the given (year, month).\n",
    "\n",
    "    Notes:\n",
    "      - If (year, month) missing entirely: returns {} (caller chooses behavior).\n",
    "      - If a node risk value is NaN: filled by fallback computed from the month panel.\n",
    "    \"\"\"\n",
    "    if junction_panel_gdf is None or len(junction_panel_gdf) == 0:\n",
    "        return {}\n",
    "\n",
    "    j = junction_panel_gdf[(junction_panel_gdf[\"year\"] == year) & (junction_panel_gdf[\"month\"] == month)].copy()\n",
    "    if j.empty:\n",
    "        return {}\n",
    "\n",
    "    if node_risk_col not in j.columns:\n",
    "        raise KeyError(f\"Missing node risk column '{node_risk_col}' in junction_panel_gdf\")\n",
    "\n",
    "    # Ensure one row per node_id-month; if multiple exist, take mean (shouldn't happen, but safe).\n",
    "    j = j.groupby(node_id_col, as_index=False)[node_risk_col].mean()\n",
    "\n",
    "    fb = _risk_fallback_value(j[node_risk_col], strategy=fallback_strategy, default=fallback)\n",
    "    j[node_risk_col] = pd.to_numeric(j[node_risk_col], errors=\"coerce\").fillna(fb)\n",
    "\n",
    "    return dict(zip(j[node_id_col].astype(str), j[node_risk_col].astype(float)))\n",
    "\n",
    "\n",
    "def _attach_node_ids_to_graph_nodes(\n",
    "    G: nx.Graph,\n",
    "    crossings_gdf: gpd.GeoDataFrame,\n",
    "    *,\n",
    "    metric_epsg: int = 32633,\n",
    "    node_id_col: str = \"node_id\",\n",
    "    max_snap_m: float = 20.0,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Map your existing 'node_id' (junctions/crossings) onto graph nodes.\n",
    "\n",
    "    We snap each crossing point to the nearest graph node (within max_snap_m).\n",
    "    Stores: G.nodes[node_key][\"node_id\"] = <id>\n",
    "\n",
    "    Critical for applying junction risk penalties.\n",
    "    \"\"\"\n",
    "    if crossings_gdf is None or len(crossings_gdf) == 0:\n",
    "        return\n",
    "\n",
    "    crossings_m = crossings_gdf.to_crs(epsg=metric_epsg).copy()\n",
    "\n",
    "    graph_nodes = list(G.nodes())\n",
    "    if len(graph_nodes) == 0:\n",
    "        return\n",
    "\n",
    "    graph_xy = np.array(graph_nodes, dtype=float)\n",
    "\n",
    "    for _, row in crossings_m.iterrows():\n",
    "        if row.geometry is None or row.geometry.is_empty:\n",
    "            continue\n",
    "\n",
    "        nid = str(row[node_id_col])\n",
    "        x, y = float(row.geometry.x), float(row.geometry.y)\n",
    "\n",
    "        d2 = (graph_xy[:, 0] - x) ** 2 + (graph_xy[:, 1] - y) ** 2\n",
    "        j = int(d2.argmin())\n",
    "        dist = float(math.sqrt(d2[j]))\n",
    "        if dist <= max_snap_m:\n",
    "            node_key = graph_nodes[j]\n",
    "            G.nodes[node_key][\"node_id\"] = nid\n",
    "\n",
    "\n",
    "def _iter_edges(G: nx.Graph):\n",
    "    \"\"\"\n",
    "    Unified iterator over edges returning (u, v, key, data) where key is None for non-multigraphs.\n",
    "    \"\"\"\n",
    "    if isinstance(G, nx.MultiGraph):\n",
    "        for u, v, k, data in G.edges(keys=True, data=True):\n",
    "            yield u, v, k, data\n",
    "    else:\n",
    "        for u, v, data in G.edges(data=True):\n",
    "            yield u, v, None, data\n",
    "\n",
    "\n",
    "def _get_edge_data_for_step(G: nx.Graph, a, b, *, weight_key: str = \"cost\") -> dict:\n",
    "    \"\"\"\n",
    "    For MultiGraph: pick the edge (between a,b) that minimizes weight_key, fallback to length_m.\n",
    "    For Graph: return edge data.\n",
    "    \"\"\"\n",
    "    if isinstance(G, nx.MultiGraph):\n",
    "        edges = G.get_edge_data(a, b)\n",
    "        if edges is None:\n",
    "            raise KeyError(f\"No edge between {a} and {b}\")\n",
    "\n",
    "        best = None\n",
    "        best_val = None\n",
    "        for _, d in edges.items():\n",
    "            val = d.get(weight_key, d.get(\"length_m\", 0.0))\n",
    "            if best is None or val < best_val:\n",
    "                best = d\n",
    "                best_val = val\n",
    "        return best\n",
    "    else:\n",
    "        d = G.get_edge_data(a, b)\n",
    "        if d is None:\n",
    "            raise KeyError(f\"No edge between {a} and {b}\")\n",
    "        return d\n",
    "\n",
    "\n",
    "# Graph construction\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class GraphBuildConfig:\n",
    "    metric_epsg: int = 32633\n",
    "    endpoint_ndigits: int = 0              # 0 => 1m rounding\n",
    "    seg_id_col: str = \"counter_name\"\n",
    "    seg_exposure_col_candidates: Tuple[str, ...] = (\"monthly_strava_trips\", \"sum_strava_total_trip_count\")\n",
    "    seg_risk_col_candidates: Tuple[str, ...] = (\"risk_accidents_per_10k_trips\", \"risk_accidents_per_trip\")\n",
    "    # if using per-trip risk, we scale to per-10k automatically for routing:\n",
    "    auto_scale_trip_risk_to_per_10k: bool = True\n",
    "    # risk fallback policy if risk is NaN:\n",
    "    risk_fallback_strategy: str = \"median\"   # median/mean/high/zero\n",
    "    risk_fallback_default: float = 0.0\n",
    "    # when exposure is 0 / missing:\n",
    "    drop_edges_with_zero_exposure: bool = True\n",
    "    keep_parallel_edges: bool = True\n",
    "\n",
    "\n",
    "def build_routing_graph_for_month(\n",
    "    segments_panel_gdf: gpd.GeoDataFrame,\n",
    "    year: int,\n",
    "    month: int,\n",
    "    *,\n",
    "    config: GraphBuildConfig = GraphBuildConfig(),\n",
    ") -> nx.Graph:\n",
    "    \"\"\"\n",
    "    Build a routing graph for one (year, month) from segment LineStrings.\n",
    "\n",
    "    Nodes: segment endpoints (rounded in metric CRS).\n",
    "    Edges carry:\n",
    "      - segment_id\n",
    "      - length_m\n",
    "      - seg_risk (scaled for routing; typically per-10k trips)\n",
    "      - geometry\n",
    "\n",
    "    Important fixes vs the earlier version:\n",
    "      - avoids mixing units by preferring per-10k risk; auto-scales per-trip risk if needed\n",
    "      - optionally drops edges with zero exposure for that month (safer than treating unknown as 0)\n",
    "      - robust fallback for missing risk (median/mean/high)\n",
    "    \"\"\"\n",
    "    df = segments_panel_gdf[(segments_panel_gdf[\"year\"] == year) & (segments_panel_gdf[\"month\"] == month)].copy()\n",
    "    if df.empty:\n",
    "        raise ValueError(f\"No segment rows found for year={year}, month={month}\")\n",
    "\n",
    "    df_m = df.to_crs(epsg=config.metric_epsg).copy()\n",
    "\n",
    "    # Pick exposure column\n",
    "    exposure_col = None\n",
    "    for c in config.seg_exposure_col_candidates:\n",
    "        if c in df_m.columns:\n",
    "            exposure_col = c\n",
    "            break\n",
    "    if exposure_col is None:\n",
    "        raise KeyError(f\"Missing exposure column. Tried: {config.seg_exposure_col_candidates}\")\n",
    "\n",
    "    # Pick risk column\n",
    "    risk_col = None\n",
    "    for c in config.seg_risk_col_candidates:\n",
    "        if c in df_m.columns:\n",
    "            risk_col = c\n",
    "            break\n",
    "    if risk_col is None:\n",
    "        raise KeyError(f\"Missing risk column. Tried: {config.seg_risk_col_candidates}\")\n",
    "\n",
    "    # Ensure uniqueness at segment-month level\n",
    "    if df_m[[config.seg_id_col, \"year\", \"month\"]].duplicated().any():\n",
    "        # This should not happen in a proper panel; fail loudly to avoid silently building duplicates\n",
    "        dups = int(df_m[[config.seg_id_col, \"year\", \"month\"]].duplicated().sum())\n",
    "        raise ValueError(f\"segments_panel_gdf has {dups} duplicate rows for (segment,year,month). Fix upstream aggregation.\")\n",
    "\n",
    "    # Optionally drop edges with zero exposure (or missing exposure)\n",
    "    df_m[exposure_col] = pd.to_numeric(df_m[exposure_col], errors=\"coerce\")\n",
    "    if config.drop_edges_with_zero_exposure:\n",
    "        df_m = df_m[df_m[exposure_col].fillna(0) > 0].copy()\n",
    "\n",
    "    # Compute fallback risk (month-specific)\n",
    "    fb = _risk_fallback_value(df_m[risk_col], strategy=config.risk_fallback_strategy, default=config.risk_fallback_default)\n",
    "\n",
    "    # Build graph\n",
    "    G = nx.MultiGraph() if config.keep_parallel_edges else nx.Graph()\n",
    "\n",
    "    for _, row in df_m.iterrows():\n",
    "        geom = row.geometry\n",
    "        if geom is None or geom.is_empty:\n",
    "            continue\n",
    "        if not isinstance(geom, LineString):\n",
    "            # If MultiLineString, take the longest component (common safe fallback)\n",
    "            try:\n",
    "                parts = list(geom.geoms)\n",
    "                parts = [p for p in parts if isinstance(p, LineString) and not p.is_empty]\n",
    "                if not parts:\n",
    "                    continue\n",
    "                geom = max(parts, key=lambda g: g.length)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        u, v = _edge_endpoints_as_node_keys(geom, ndigits=config.endpoint_ndigits)\n",
    "        seg_id = str(row[config.seg_id_col])\n",
    "        length_m = float(geom.length)\n",
    "\n",
    "        seg_risk = pd.to_numeric(row[risk_col], errors=\"coerce\")\n",
    "        if pd.isna(seg_risk):\n",
    "            seg_risk = fb\n",
    "        seg_risk = float(seg_risk)\n",
    "\n",
    "        # If only per-trip risk exists, scale to per-10k for routing stability\n",
    "        if (risk_col == \"risk_accidents_per_trip\") and config.auto_scale_trip_risk_to_per_10k:\n",
    "            seg_risk *= 10_000.0\n",
    "\n",
    "        if u not in G:\n",
    "            G.add_node(u, x=u[0], y=u[1])\n",
    "        if v not in G:\n",
    "            G.add_node(v, x=v[0], y=v[1])\n",
    "\n",
    "        G.add_edge(\n",
    "            u,\n",
    "            v,\n",
    "            segment_id=seg_id,\n",
    "            length_m=length_m,\n",
    "            seg_risk=seg_risk,\n",
    "            geometry=geom,\n",
    "        )\n",
    "\n",
    "    if G.number_of_edges() == 0:\n",
    "        raise ValueError(f\"Graph has 0 edges for year={year}, month={month} after filtering. Check exposure coverage and filters.\")\n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "# Cost attributes (routing objective)\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class CostConfig:\n",
    "    alpha: float = 1.0   # weight on length (meters)\n",
    "    beta: float = 1.0    # weight on segment risk (expected to be per-10k scale)\n",
    "    gamma: float = 0.0   # weight on junction/node risk (optional; expected per-10k scale)\n",
    "\n",
    "\n",
    "def add_cost_attributes(\n",
    "    G: nx.Graph,\n",
    "    *,\n",
    "    cost_cfg: CostConfig = CostConfig(),\n",
    "    node_risk_by_node_id: Optional[Dict[str, float]] = None,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Adds 'cost' and 'node_penalty' to each edge.\n",
    "\n",
    "    cost = alpha*length_m + beta*seg_risk + gamma*node_penalty\n",
    "\n",
    "    node_penalty is an edge-local approximation:\n",
    "      node_penalty = 0.5*(risk(node_u)+risk(node_v))\n",
    "\n",
    "    Fixes vs earlier version:\n",
    "      - correct iteration for Graph vs MultiGraph\n",
    "      - safe default for missing node ids\n",
    "      - stores into edge dict consistently\n",
    "    \"\"\"\n",
    "    if node_risk_by_node_id is None:\n",
    "        node_risk_by_node_id = {}\n",
    "\n",
    "    for u, v, k, data in _iter_edges(G):\n",
    "        length_m = float(data.get(\"length_m\", 0.0))\n",
    "        seg_risk = float(data.get(\"seg_risk\", 0.0))\n",
    "\n",
    "        node_penalty = 0.0\n",
    "        if cost_cfg.gamma != 0.0:\n",
    "            uid = G.nodes[u].get(\"node_id\")\n",
    "            vid = G.nodes[v].get(\"node_id\")\n",
    "            ru = float(node_risk_by_node_id.get(uid, 0.0)) if uid is not None else 0.0\n",
    "            rv = float(node_risk_by_node_id.get(vid, 0.0)) if vid is not None else 0.0\n",
    "            node_penalty = 0.5 * (ru + rv)\n",
    "\n",
    "        cost = cost_cfg.alpha * length_m + cost_cfg.beta * seg_risk + cost_cfg.gamma * node_penalty\n",
    "\n",
    "        if isinstance(G, nx.MultiGraph):\n",
    "            G.edges[u, v, k][\"node_penalty\"] = float(node_penalty)\n",
    "            G.edges[u, v, k][\"cost\"] = float(cost)\n",
    "        else:\n",
    "            data[\"node_penalty\"] = float(node_penalty)\n",
    "            data[\"cost\"] = float(cost)\n",
    "\n",
    "\n",
    "# Routing utilities\n",
    "\n",
    "def nearest_graph_node(\n",
    "    G: nx.Graph,\n",
    "    lon: float,\n",
    "    lat: float,\n",
    "    *,\n",
    "    metric_epsg: int = 32633,\n",
    ") -> NodeKey:\n",
    "    \"\"\"\n",
    "    Snap an (lon, lat) point to the closest graph node.\n",
    "    Brute force over nodes (OK for moderate graphs). Replace with KDTree if needed.\n",
    "    \"\"\"\n",
    "    p = gpd.GeoSeries([Point(lon, lat)], crs=\"EPSG:4326\").to_crs(epsg=metric_epsg).iloc[0]\n",
    "    x, y = float(p.x), float(p.y)\n",
    "\n",
    "    nodes = list(G.nodes())\n",
    "    if len(nodes) == 0:\n",
    "        raise ValueError(\"Graph has no nodes\")\n",
    "\n",
    "    xy = np.array(nodes, dtype=float)\n",
    "    d2 = (xy[:, 0] - x) ** 2 + (xy[:, 1] - y) ** 2\n",
    "    idx = int(d2.argmin())\n",
    "    return nodes[idx]\n",
    "\n",
    "\n",
    "def route_stats(G: nx.Graph, path: List[NodeKey]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Summarize route length and risk along a path of nodes.\n",
    "\n",
    "    For MultiGraph, uses the minimum-'cost' edge between consecutive nodes if present.\n",
    "    \"\"\"\n",
    "    total_len = 0.0\n",
    "    total_seg_risk = 0.0\n",
    "    total_cost = 0.0\n",
    "    total_node_penalty = 0.0\n",
    "\n",
    "    for a, b in zip(path[:-1], path[1:]):\n",
    "        d = _get_edge_data_for_step(G, a, b, weight_key=\"cost\")\n",
    "\n",
    "        total_len += float(d.get(\"length_m\", 0.0))\n",
    "        total_seg_risk += float(d.get(\"seg_risk\", 0.0))\n",
    "        total_node_penalty += float(d.get(\"node_penalty\", 0.0))\n",
    "        total_cost += float(d.get(\"cost\", 0.0))\n",
    "\n",
    "    return {\n",
    "        \"length_m\": total_len,\n",
    "        \"seg_risk_sum\": total_seg_risk,\n",
    "        \"node_penalty_sum\": total_node_penalty,\n",
    "        \"cost_sum\": total_cost,\n",
    "    }\n",
    "\n",
    "\n",
    "def shortest_path_by(G: nx.Graph, source: NodeKey, target: NodeKey, weight: str) -> List[NodeKey]:\n",
    "    return nx.shortest_path(G, source=source, target=target, weight=weight, method=\"dijkstra\")\n",
    "\n",
    "\n",
    "def constrained_min_risk_route(\n",
    "    G: nx.Graph,\n",
    "    source: NodeKey,\n",
    "    target: NodeKey,\n",
    "    *,\n",
    "    eps: float = 0.10,                 # allow up to +10% distance over shortest\n",
    "    length_attr: str = \"length_m\",\n",
    "    risk_attr: str = \"seg_risk\",\n",
    "    lambdas: Optional[List[float]] = None,\n",
    ") -> List[NodeKey]:\n",
    "    \"\"\"\n",
    "    Minimize risk subject to length <= (1+eps) * shortest_length.\n",
    "\n",
    "    Implementation: parametric sweep with a Lagrangian:\n",
    "      minimize (risk + lambda * length)\n",
    "    and select the best path that satisfies the length constraint.\n",
    "\n",
    "    Fixes vs earlier version:\n",
    "      - correct edge iteration for Graph vs MultiGraph\n",
    "      - uses actual per-edge length/risk attributes (not route_stats hardcoded)\n",
    "      - does not rely on 'cost' existing\n",
    "    \"\"\"\n",
    "    shortest_len_path = shortest_path_by(G, source, target, weight=length_attr)\n",
    "    shortest_len = route_stats(G, shortest_len_path)[\"length_m\"]\n",
    "    max_len = (1.0 + eps) * shortest_len\n",
    "\n",
    "    if lambdas is None:\n",
    "        lambdas = [0.0, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1.0, 10.0, 100.0, 1000.0]\n",
    "\n",
    "    best_path = None\n",
    "    best_risk = None\n",
    "    best_len = None\n",
    "\n",
    "    # Build temporary combined weight on edges: risk + lambda*length\n",
    "    for lam in lambdas:\n",
    "        for u, v, k, d in _iter_edges(G):\n",
    "            comb = float(d.get(risk_attr, 0.0)) + lam * float(d.get(length_attr, 0.0))\n",
    "            if isinstance(G, nx.MultiGraph):\n",
    "                G.edges[u, v, k][\"_comb\"] = comb\n",
    "            else:\n",
    "                d[\"_comb\"] = comb\n",
    "\n",
    "        p = shortest_path_by(G, source, target, weight=\"_comb\")\n",
    "        st = route_stats(G, p)\n",
    "        if st[\"length_m\"] <= max_len:\n",
    "            if best_path is None or st[\"seg_risk_sum\"] < best_risk or (st[\"seg_risk_sum\"] == best_risk and st[\"length_m\"] < best_len):\n",
    "                best_path = p\n",
    "                best_risk = st[\"seg_risk_sum\"]\n",
    "                best_len = st[\"length_m\"]\n",
    "\n",
    "    return best_path if best_path is not None else shortest_len_path\n",
    "\n",
    "\n",
    "# End-to-end wrapper\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class RoutingMonthArtifacts:\n",
    "    G: nx.Graph\n",
    "    year: int\n",
    "    month: int\n",
    "    segment_risk_col_used: str\n",
    "    node_risk_col_used: Optional[str]\n",
    "    notes: str\n",
    "\n",
    "\n",
    "def build_graph_with_costs_for_month(\n",
    "    segments_panel_gdf: gpd.GeoDataFrame,\n",
    "    year: int,\n",
    "    month: int,\n",
    "    *,\n",
    "    crossings_gdf: Optional[gpd.GeoDataFrame] = None,\n",
    "    junction_panel_gdf: Optional[gpd.GeoDataFrame] = None,\n",
    "    graph_cfg: GraphBuildConfig = GraphBuildConfig(),\n",
    "    cost_cfg: CostConfig = CostConfig(alpha=1.0, beta=1.0, gamma=0.0),\n",
    "    node_risk_col: str = \"risk_accidents_per_10k_trips\",\n",
    "    node_snap_m: float = 30.0,\n",
    "    node_risk_fallback_strategy: str = \"median\",\n",
    "    node_risk_fallback_default: float = 0.0,\n",
    ") -> RoutingMonthArtifacts:\n",
    "    \"\"\"\n",
    "    Build the month-specific routing graph and attach cost attributes.\n",
    "    This is the piece that makes the routing claim in your paper reproducible.\n",
    "\n",
    "    What it does:\n",
    "      1) Build graph edges from segment panel for (year, month)\n",
    "      2) Optionally attach node_ids to graph nodes by snapping crossings_gdf points\n",
    "      3) Optionally build node risk lookup from junction_panel_gdf for (year, month)\n",
    "      4) Add edge costs combining length + segment risk + optional junction penalties\n",
    "\n",
    "    Critical: This is where junction risk actually gets integrated (if gamma != 0).\n",
    "    \"\"\"\n",
    "    G = build_routing_graph_for_month(segments_panel_gdf, year, month, config=graph_cfg)\n",
    "\n",
    "    node_risk_lookup = {}\n",
    "    used_node_risk_col = None\n",
    "    notes = []\n",
    "\n",
    "    if cost_cfg.gamma != 0.0:\n",
    "        if crossings_gdf is None or junction_panel_gdf is None:\n",
    "            notes.append(\"gamma!=0 but crossings_gdf/junction_panel_gdf not provided -> junction penalty disabled.\")\n",
    "        else:\n",
    "            _attach_node_ids_to_graph_nodes(\n",
    "                G,\n",
    "                crossings_gdf,\n",
    "                metric_epsg=graph_cfg.metric_epsg,\n",
    "                node_id_col=\"node_id\",\n",
    "                max_snap_m=node_snap_m,\n",
    "            )\n",
    "            node_risk_lookup = _build_node_risk_lookup(\n",
    "                junction_panel_gdf,\n",
    "                year,\n",
    "                month,\n",
    "                node_id_col=\"node_id\",\n",
    "                node_risk_col=node_risk_col,\n",
    "                fallback=node_risk_fallback_default,\n",
    "                fallback_strategy=node_risk_fallback_strategy,\n",
    "            )\n",
    "            used_node_risk_col = node_risk_col\n",
    "\n",
    "            attached = sum(1 for n in G.nodes if \"node_id\" in G.nodes[n])\n",
    "            notes.append(f\"Attached node_id to {attached}/{G.number_of_nodes()} graph nodes (snap<= {node_snap_m}m).\")\n",
    "\n",
    "    add_cost_attributes(G, cost_cfg=cost_cfg, node_risk_by_node_id=node_risk_lookup)\n",
    "\n",
    "    # Determine which segment risk column was used (for transparency)\n",
    "    seg_risk_col_used = None\n",
    "    for c in graph_cfg.seg_risk_col_candidates:\n",
    "        if c in segments_panel_gdf.columns:\n",
    "            seg_risk_col_used = c\n",
    "            break\n",
    "    if seg_risk_col_used is None:\n",
    "        seg_risk_col_used = \"(unknown)\"\n",
    "\n",
    "    # If per-trip risk was used but scaled, note it\n",
    "    if seg_risk_col_used == \"risk_accidents_per_trip\" and graph_cfg.auto_scale_trip_risk_to_per_10k:\n",
    "        notes.append(\"Scaled segment risk_accidents_per_trip by 10,000 for routing stability (per-10k trips).\")\n",
    "\n",
    "    return RoutingMonthArtifacts(\n",
    "        G=G,\n",
    "        year=year,\n",
    "        month=month,\n",
    "        segment_risk_col_used=seg_risk_col_used,\n",
    "        node_risk_col_used=used_node_risk_col,\n",
    "        notes=\" \".join(notes).strip(),\n",
    "    )\n",
    "\n",
    "\n",
    "# Verification utilities\n",
    "\n",
    "def verify_graph_sanity(\n",
    "    artifacts: RoutingMonthArtifacts,\n",
    "    *,\n",
    "    expect_junction_penalties: bool = False,\n",
    ") -> Dict[str, Union[int, float, str]]:\n",
    "    \"\"\"\n",
    "    Quick, paper-friendly sanity checks:\n",
    "      - graph non-empty\n",
    "      - risk magnitude reasonable\n",
    "      - cost depends on risk (not strictly provable here, but we report ranges)\n",
    "      - node ids attached when expected\n",
    "    \"\"\"\n",
    "    G = artifacts.G\n",
    "\n",
    "    seg_risks = [float(d.get(\"seg_risk\", 0.0)) for _, _, _, d in _iter_edges(G)]\n",
    "    lengths = [float(d.get(\"length_m\", 0.0)) for _, _, _, d in _iter_edges(G)]\n",
    "    costs = [float(d.get(\"cost\", 0.0)) for _, _, _, d in _iter_edges(G)]\n",
    "    node_pen = [float(d.get(\"node_penalty\", 0.0)) for _, _, _, d in _iter_edges(G)]\n",
    "\n",
    "    attached_nodes = sum(1 for n in G.nodes if \"node_id\" in G.nodes[n])\n",
    "\n",
    "    out = {\n",
    "        \"n_nodes\": G.number_of_nodes(),\n",
    "        \"n_edges\": G.number_of_edges(),\n",
    "        \"seg_risk_min\": float(np.min(seg_risks)) if seg_risks else float(\"nan\"),\n",
    "        \"seg_risk_median\": float(np.median(seg_risks)) if seg_risks else float(\"nan\"),\n",
    "        \"seg_risk_max\": float(np.max(seg_risks)) if seg_risks else float(\"nan\"),\n",
    "        \"length_median\": float(np.median(lengths)) if lengths else float(\"nan\"),\n",
    "        \"cost_median\": float(np.median(costs)) if costs else float(\"nan\"),\n",
    "        \"node_penalty_nonzero_share\": float(np.mean(np.array(node_pen) > 0)) if node_pen else 0.0,\n",
    "        \"graph_node_ids_attached\": attached_nodes,\n",
    "        \"notes\": artifacts.notes,\n",
    "    }\n",
    "\n",
    "    if expect_junction_penalties and attached_nodes == 0:\n",
    "        out[\"warning\"] = \"Expected junction penalties, but no graph nodes have node_id attached. Check snapping tolerance and CRS.\"\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afde4ad0",
   "metadata": {},
   "source": [
    "## Run routing for one OD pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a543dd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_nodes': 3074, 'n_edges': 4395, 'seg_risk_min': 0.0, 'seg_risk_median': 0.0, 'seg_risk_max': 4000.0, 'length_median': 371.8095742150327, 'cost_median': 373.6283729421776, 'node_penalty_nonzero_share': 0.08964732650739476, 'graph_node_ids_attached': 2882, 'notes': 'Attached node_id to 2882/3074 graph nodes (snap<= 20.0m).'}\n",
      "Shortest length: {'length_m': 6599.195578923747, 'seg_risk_sum': 0.0, 'node_penalty_sum': 0.0, 'cost_sum': 6599.195578923747}\n",
      "Constrained min-risk: {'length_m': 6599.195578923747, 'seg_risk_sum': 0.0, 'node_penalty_sum': 0.0, 'cost_sum': 6599.195578923747}\n"
     ]
    }
   ],
   "source": [
    "# Run routing for ONE OD pair\n",
    "\n",
    "def run_one_od_routing(\n",
    "    *,\n",
    "    segments_panel_gdf: gpd.GeoDataFrame,\n",
    "    crossings_gdf: gpd.GeoDataFrame,\n",
    "    junction_panel_gdf: gpd.GeoDataFrame,\n",
    "    year: int,\n",
    "    month: int,\n",
    "    origin_lonlat: tuple[float, float],\n",
    "    dest_lonlat: tuple[float, float],\n",
    "    eps: float = 0.10,\n",
    "    # cost weights\n",
    "    alpha: float = 1.0,\n",
    "    beta: float = 1.0,\n",
    "    gamma: float = 0.0,\n",
    "    metric_epsg: int = 32633,\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds the month graph, snaps OD to graph nodes, and returns:\n",
    "      - shortest-by-length route\n",
    "      - constrained min-risk route within (1+eps) distance\n",
    "\n",
    "    origin_lonlat/dest_lonlat are (lon, lat) in EPSG:4326.\n",
    "    \"\"\"\n",
    "    artifacts = build_graph_with_costs_for_month(\n",
    "        segments_panel_gdf,\n",
    "        year,\n",
    "        month,\n",
    "        crossings_gdf=crossings_gdf,\n",
    "        junction_panel_gdf=junction_panel_gdf,\n",
    "        graph_cfg=GraphBuildConfig(metric_epsg=metric_epsg),\n",
    "        cost_cfg=CostConfig(alpha=alpha, beta=beta, gamma=gamma),\n",
    "        node_snap_m=20.0,\n",
    "    )\n",
    "\n",
    "    sanity = verify_graph_sanity(artifacts, expect_junction_penalties=(gamma != 0.0))\n",
    "\n",
    "    G = artifacts.G\n",
    "    o_lon, o_lat = origin_lonlat\n",
    "    d_lon, d_lat = dest_lonlat\n",
    "\n",
    "    src = nearest_graph_node(G, o_lon, o_lat, metric_epsg=metric_epsg)\n",
    "    dst = nearest_graph_node(G, d_lon, d_lat, metric_epsg=metric_epsg)\n",
    "\n",
    "    # Baseline: shortest distance\n",
    "    p_len = shortest_path_by(G, src, dst, weight=\"length_m\")\n",
    "    st_len = route_stats(G, p_len)\n",
    "\n",
    "    # Constrained min-risk (risk attribute is 'seg_risk' on edges)\n",
    "    p_safe = constrained_min_risk_route(G, src, dst, eps=eps, length_attr=\"length_m\", risk_attr=\"seg_risk\")\n",
    "    st_safe = route_stats(G, p_safe)\n",
    "\n",
    "    # Also compute shortest-by-cost if you want (mixed objective)\n",
    "    p_cost = shortest_path_by(G, src, dst, weight=\"cost\")\n",
    "    st_cost = route_stats(G, p_cost)\n",
    "\n",
    "    return {\n",
    "        \"graph_sanity\": sanity,\n",
    "        \"origin_node\": src,\n",
    "        \"dest_node\": dst,\n",
    "        \"shortest_length_path\": p_len,\n",
    "        \"shortest_length_stats\": st_len,\n",
    "        \"constrained_min_risk_path\": p_safe,\n",
    "        \"constrained_min_risk_stats\": st_safe,\n",
    "        \"shortest_cost_path\": p_cost,\n",
    "        \"shortest_cost_stats\": st_cost,\n",
    "        \"notes\": artifacts.notes,\n",
    "    }\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "result = run_one_od_routing(\n",
    "    segments_panel_gdf=segments_df,\n",
    "    crossings_gdf=crossings_gdf,\n",
    "    junction_panel_gdf=junction_df,\n",
    "    year=2021, month=6,\n",
    "    origin_lonlat=(13.3777, 52.5163),\n",
    "    dest_lonlat=(13.4541, 52.5110),\n",
    "    eps=0.10,\n",
    "    alpha=1.0, beta=1.0, gamma=0.2,\n",
    ")\n",
    "print(result[\"graph_sanity\"])\n",
    "print(\"Shortest length:\", result[\"shortest_length_stats\"])\n",
    "print(\"Constrained min-risk:\", result[\"constrained_min_risk_stats\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5effb40",
   "metadata": {},
   "source": [
    "## Evaluation loop over many OD pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "807a5030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_nodes': 3074, 'n_edges': 4395, 'seg_risk_min': 0.0, 'seg_risk_median': 0.0, 'seg_risk_max': 4000.0, 'length_median': 371.8095742150327, 'cost_median': 373.6283729421776, 'node_penalty_nonzero_share': 0.08964732650739476, 'graph_node_ids_attached': 2882, 'notes': 'Attached node_id to 2882/3074 graph nodes (snap<= 20.0m).'}\n",
      "       safe_extra_len_pct  safe_risk_reduction_pct\n",
      "count          300.000000               272.000000\n",
      "mean             0.040958                 0.825061\n",
      "std              0.030359                 0.315402\n",
      "min              0.000000                 0.000000\n",
      "25%              0.013169                 0.797813\n",
      "50%              0.040338                 1.000000\n",
      "75%              0.065013                 1.000000\n",
      "max              0.099990                 1.000000\n"
     ]
    }
   ],
   "source": [
    "# Extensive evaluation loop over MANY OD pairs\n",
    "\n",
    "def _sample_reachable_od_pairs(\n",
    "    G: nx.Graph,\n",
    "    n_pairs: int,\n",
    "    *,\n",
    "    seed: int = 7,\n",
    "    min_euclid_m: float = 1500.0,\n",
    "    max_tries: int = 200000,\n",
    ") -> list[tuple[tuple[float, float], tuple[float, float]]]:\n",
    "    \"\"\"\n",
    "    Samples OD pairs as graph nodes (not lon/lat) ensuring:\n",
    "      - O != D\n",
    "      - straight-line distance between nodes >= min_euclid_m\n",
    "      - O and D are connected (same component)\n",
    "\n",
    "    Returns list of (source_node, target_node) where each node is the (x,y) NodeKey.\n",
    "    \"\"\"\n",
    "    rng = random.Random(seed)\n",
    "    nodes = list(G.nodes())\n",
    "    if len(nodes) < 2:\n",
    "        return []\n",
    "\n",
    "    # Precompute component id for each node to ensure reachability\n",
    "    comp_map = {}\n",
    "    for cid, comp in enumerate(nx.connected_components(G.to_undirected())):\n",
    "        for n in comp:\n",
    "            comp_map[n] = cid\n",
    "\n",
    "    pairs = []\n",
    "    tries = 0\n",
    "    while len(pairs) < n_pairs and tries < max_tries:\n",
    "        tries += 1\n",
    "        a = rng.choice(nodes)\n",
    "        b = rng.choice(nodes)\n",
    "        if a == b:\n",
    "            continue\n",
    "        if comp_map.get(a) != comp_map.get(b):\n",
    "            continue\n",
    "\n",
    "        dx = float(a[0] - b[0])\n",
    "        dy = float(a[1] - b[1])\n",
    "        if math.sqrt(dx * dx + dy * dy) < min_euclid_m:\n",
    "            continue\n",
    "\n",
    "        pairs.append((a, b))\n",
    "\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def evaluate_many_od_pairs(\n",
    "    *,\n",
    "    segments_panel_gdf: gpd.GeoDataFrame,\n",
    "    crossings_gdf: gpd.GeoDataFrame,\n",
    "    junction_panel_gdf: gpd.GeoDataFrame,\n",
    "    year: int,\n",
    "    month: int,\n",
    "    n_pairs: int = 200,\n",
    "    eps: float = 0.10,\n",
    "    # cost weights\n",
    "    alpha: float = 1.0,\n",
    "    beta: float = 1.0,\n",
    "    gamma: float = 0.0,\n",
    "    metric_epsg: int = 32633,\n",
    "    seed: int = 7,\n",
    "    min_euclid_m: float = 1500.0,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Builds month graph once, samples many reachable OD pairs on the graph,\n",
    "    and evaluates:\n",
    "      - shortest-by-length\n",
    "      - shortest-by-cost\n",
    "      - constrained-min-risk route (risk subject to length constraint)\n",
    "\n",
    "    Returns a DataFrame with per-OD statistics + summary columns suitable for paper plots.\n",
    "    \"\"\"\n",
    "    artifacts = build_graph_with_costs_for_month(\n",
    "        segments_panel_gdf,\n",
    "        year,\n",
    "        month,\n",
    "        crossings_gdf=crossings_gdf,\n",
    "        junction_panel_gdf=junction_panel_gdf,\n",
    "        graph_cfg=GraphBuildConfig(metric_epsg=metric_epsg),\n",
    "        cost_cfg=CostConfig(alpha=alpha, beta=beta, gamma=gamma),\n",
    "        node_snap_m=20.0,\n",
    "    )\n",
    "\n",
    "    sanity = verify_graph_sanity(artifacts, expect_junction_penalties=(gamma != 0.0))\n",
    "    G = artifacts.G\n",
    "\n",
    "    # Sample OD pairs directly from graph nodes to avoid repeated lon/lat snapping and to ensure reachability\n",
    "    od_pairs = _sample_reachable_od_pairs(G, n_pairs, seed=seed, min_euclid_m=min_euclid_m)\n",
    "    if len(od_pairs) == 0:\n",
    "        raise ValueError(\"Could not sample any reachable OD pairs. Check graph connectivity / size.\")\n",
    "\n",
    "    rows = []\n",
    "    for i, (src, dst) in enumerate(od_pairs):\n",
    "        # 1) shortest length\n",
    "        p_len = shortest_path_by(G, src, dst, weight=\"length_m\")\n",
    "        st_len = route_stats(G, p_len)\n",
    "        shortest_len = st_len[\"length_m\"]\n",
    "        max_len = (1.0 + eps) * shortest_len\n",
    "\n",
    "        # 2) shortest cost\n",
    "        p_cost = shortest_path_by(G, src, dst, weight=\"cost\")\n",
    "        st_cost = route_stats(G, p_cost)\n",
    "\n",
    "        # 3) constrained min-risk (risk = seg_risk)\n",
    "        p_safe = constrained_min_risk_route(G, src, dst, eps=eps, length_attr=\"length_m\", risk_attr=\"seg_risk\")\n",
    "        st_safe = route_stats(G, p_safe)\n",
    "\n",
    "        rows.append({\n",
    "            \"pair_idx\": i,\n",
    "            \"src_x\": float(src[0]),\n",
    "            \"src_y\": float(src[1]),\n",
    "            \"dst_x\": float(dst[0]),\n",
    "            \"dst_y\": float(dst[1]),\n",
    "\n",
    "            \"shortest_len_m\": float(st_len[\"length_m\"]),\n",
    "            \"shortest_risk_sum\": float(st_len[\"seg_risk_sum\"]),\n",
    "            \"shortest_cost_sum\": float(st_len[\"cost_sum\"]),\n",
    "\n",
    "            \"costroute_len_m\": float(st_cost[\"length_m\"]),\n",
    "            \"costroute_risk_sum\": float(st_cost[\"seg_risk_sum\"]),\n",
    "            \"costroute_cost_sum\": float(st_cost[\"cost_sum\"]),\n",
    "\n",
    "            \"safe_len_m\": float(st_safe[\"length_m\"]),\n",
    "            \"safe_risk_sum\": float(st_safe[\"seg_risk_sum\"]),\n",
    "            \"safe_cost_sum\": float(st_safe[\"cost_sum\"]),\n",
    "\n",
    "            \"len_constraint_max_m\": float(max_len),\n",
    "            \"safe_feasible\": bool(st_safe[\"length_m\"] <= max_len + 1e-6),\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Convenience + risk trade-offs relative to shortest distance route\n",
    "    df[\"safe_extra_len_pct\"] = (df[\"safe_len_m\"] - df[\"shortest_len_m\"]) / df[\"shortest_len_m\"]\n",
    "    df[\"safe_risk_reduction_pct\"] = (df[\"shortest_risk_sum\"] - df[\"safe_risk_sum\"]) / df[\"shortest_risk_sum\"].replace(0, np.nan)\n",
    "\n",
    "    df[\"cost_extra_len_pct\"] = (df[\"costroute_len_m\"] - df[\"shortest_len_m\"]) / df[\"shortest_len_m\"]\n",
    "    df[\"cost_risk_reduction_pct\"] = (df[\"shortest_risk_sum\"] - df[\"costroute_risk_sum\"]) / df[\"shortest_risk_sum\"].replace(0, np.nan)\n",
    "\n",
    "    # Attach run metadata for reproducibility\n",
    "    df.attrs[\"graph_sanity\"] = sanity\n",
    "    df.attrs[\"notes\"] = artifacts.notes\n",
    "    df.attrs[\"year\"] = year\n",
    "    df.attrs[\"month\"] = month\n",
    "    df.attrs[\"eps\"] = eps\n",
    "    df.attrs[\"alpha\"] = alpha\n",
    "    df.attrs[\"beta\"] = beta\n",
    "    df.attrs[\"gamma\"] = gamma\n",
    "    df.attrs[\"n_pairs_requested\"] = n_pairs\n",
    "    df.attrs[\"n_pairs_used\"] = len(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "eval_df = evaluate_many_od_pairs(\n",
    "    segments_panel_gdf=segments_df,\n",
    "    crossings_gdf=crossings_gdf,\n",
    "    junction_panel_gdf=junction_df,\n",
    "    year=2021, month=6,\n",
    "    n_pairs=300,\n",
    "    eps=0.10,\n",
    "    alpha=1.0, beta=1.0, gamma=0.2,\n",
    "    seed=7,\n",
    "    min_euclid_m=2000.0,\n",
    ")\n",
    "print(eval_df.attrs[\"graph_sanity\"])\n",
    "print(eval_df[[\"safe_extra_len_pct\", \"safe_risk_reduction_pct\"]].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbff1429",
   "metadata": {},
   "source": [
    "## Loop routing over all months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e849d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for (year, month), _ in segments_df.groupby([\"year\", \"month\"]):\n",
    "    eval_df = evaluate_many_od_pairs(\n",
    "        segments_panel_gdf=segments_df,\n",
    "        crossings_gdf=crossings_gdf,\n",
    "        junction_panel_gdf=junction_df,\n",
    "        year=year,\n",
    "        month=month,\n",
    "        n_pairs=200,\n",
    "        eps=0.10,\n",
    "        alpha=1.0,\n",
    "        beta=1.0,\n",
    "        gamma=0.2,\n",
    "    )\n",
    "    eval_df[\"year\"] = year\n",
    "    eval_df[\"month\"] = month\n",
    "    results.append(eval_df)\n",
    "\n",
    "all_months_eval = pd.concat(results, ignore_index=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
