{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ca5dcc1",
   "metadata": {},
   "source": [
    "# Bicycle Accidents x Geo Data x Strava Exposure Merge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53131ecb",
   "metadata": {},
   "source": [
    "#### Import some standard libraries and helper scripts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e02390a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure project root is importable when running from notebooks/\n",
    "_project_root = Path.cwd().resolve()\n",
    "if not (_project_root / \"src\").exists() and (_project_root.parent / \"src\").exists():\n",
    "    _project_root = _project_root.parent\n",
    "sys.path.insert(0, str(_project_root))\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optional: reload project modules without restarting the kernel\n",
    "import importlib\n",
    "import src.accidents as accidents\n",
    "import src.segments as segments\n",
    "import src.strava_exposure as strava_exposure\n",
    "import src.panels as panels\n",
    "import src.nodes as nodes\n",
    "for _m in (accidents, segments, strava_exposure, panels, nodes):\n",
    "    importlib.reload(_m)\n",
    "\n",
    "from src.accidents import (\n",
    "    ACCIDENT_COLUMNS_EN,\n",
    "    assign_accidents_to_nearest_segment,\n",
    "    load_accidents_raw,\n",
    "    prepare_accidents_bike_berlin,\n",
    ")\n",
    "from src.segments import load_segment_geometry\n",
    "from src.strava_exposure import (\n",
    "    build_exposure_panel_segment_year_month,\n",
    "    column_stability_summary,\n",
    "    load_strava_berlin_data,\n",
    ")\n",
    "from src.panels import (\n",
    "    aggregate_accidents_segment_year_month_rich,\n",
    "    build_core_risk_panel,\n",
    "    merge_exposure_and_accidents,\n",
    "    sanity_check_merge,\n",
    ")\n",
    "from src.nodes import (\n",
    "    assign_accidents_to_nearest_crossing,\n",
    "    build_node_exposure_panel_from_segments,\n",
    "    build_node_risk_panel,\n",
    "    build_nodes_from_segment_endpoints,\n",
    "    cluster_nodes_snap_grid,\n",
    "    select_crossings_by_degree,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51cf230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdcaf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel cleanup: drop old variables from pre-refactor runs\n",
    "# (Equivalent to a kernel restart for the pipeline variables.)\n",
    "import gc\n",
    "import sys\n",
    "\n",
    "# Ensure the removed module isn't lingering in memory\n",
    "sys.modules.pop(\"src.merge_datasets\", None)\n",
    "\n",
    "# Drop previously computed objects/dataframes so we don't keep stale types around\n",
    "for _name in [\n",
    "    \"seg\",\n",
    "    \"clustering\",\n",
    "    \"accidents_raw\",\n",
    "    \"accidents_bike_berlin\",\n",
    "    \"segment_geo_gdf\",\n",
    "    \"segment_static\",\n",
    "    \"strava_berlin_data\",\n",
    "    \"summary_df\",\n",
    "    \"final_exposure_ym\",\n",
    "    \"accidents_agg_ym_rich\",\n",
    "    \"merged_accidents_strava_ym\",\n",
    "    \"core_panel\",\n",
    "    \"nodes_raw\",\n",
    "    \"node_points\",\n",
    "    \"crossings_gdf\",\n",
    "    \"crossing_ids\",\n",
    "    \"segment_node_map\",\n",
    "    \"node_exposure_ym\",\n",
    "    \"node_panel_ym\",\n",
    "    \"acc_node\",\n",
    "    \"acc_node_ym\",\n",
    "    \"joined_nearest_unique\",\n",
    "    \"stats\",\n",
    "    \"merge_keys\",\n",
    "    \"min_year\",\n",
    "    \"max_year\",\n",
    "    \"out_dir\",\n",
    "    \"out_path\",\n",
    "]:\n",
    "    globals().pop(_name, None)\n",
    "\n",
    "gc.collect()\n",
    "print(\"kernel_cleanup_done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b32e584",
   "metadata": {},
   "source": [
    "## Bicycle data for Berlin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a60e2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "accidents_raw = load_accidents_raw()\n",
    "print(\"Raw accidents shape:\", accidents_raw.shape)\n",
    "\n",
    "accident_columns_en = ACCIDENT_COLUMNS_EN\n",
    "\n",
    "accidents_bike_berlin = prepare_accidents_bike_berlin(accidents_raw, column_map=accident_columns_en)\n",
    "print(f\"Filtered to bicycle accidents in Berlin -> shape: {accidents_bike_berlin.shape}\")\n",
    "\n",
    "# Free big raw dataframe early to keep memory low for Strava aggregation\n",
    "del accidents_raw\n",
    "gc.collect()\n",
    "\n",
    "accidents_bike_berlin.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a7c85c",
   "metadata": {},
   "source": [
    "## Rename columns to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3bb075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column map is now maintained in src.accidents\n",
    "accident_columns_en = ACCIDENT_COLUMNS_EN\n",
    "\n",
    "# (Optional) quick per-column uniqueness scan\n",
    "for col in accidents_bike_berlin.columns:\n",
    "    uniq_cnt = accidents_bike_berlin[col].nunique(dropna=True)\n",
    "    first_vals = accidents_bike_berlin[col].head(5).tolist()\n",
    "    print(f\"{col}: uniques={uniq_cnt}; first5={first_vals}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b926bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg = load_segment_geometry(canonical_crs=\"EPSG:32633\")\n",
    "CANONICAL_CRS = seg.canonical_crs\n",
    "\n",
    "segment_geo_gdf = seg.segments_gdf\n",
    "segment_static = seg.segment_static\n",
    "\n",
    "segment_geo_gdf.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3fda65",
   "metadata": {},
   "source": [
    "## Spatial Join: Accidents with Strava data (code from Luise and Eric) + edited by Tobi to achieve canonical geometry data\n",
    "\n",
    "\n",
    "### Attempt 2: Use sjoin_nearest to assign exactly one (the nearest) segment to each accident\n",
    "Challenges:\n",
    "* need to find the right maximum distance so accidents that are not on a segment are not assigned to one.\n",
    "* assigns two segments if their distance is equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d88c2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign each accident to exactly one nearest segment (within max_distance)\n",
    "joined_nearest_unique = assign_accidents_to_nearest_segment(\n",
    "    accidents_bike_berlin,\n",
    "    segment_geo_gdf,\n",
    "    canonical_crs=CANONICAL_CRS,\n",
    "    max_distance_m=10,\n",
    ")\n",
    "\n",
    "print(f\"Total accidents: {len(accidents_bike_berlin)}\")\n",
    "print(f\"Total bike network Strava segments: {len(segment_geo_gdf)}\")\n",
    "print(f\"Unique Strava segments in matched dataset: {joined_nearest_unique['counter_name'].nunique()}\")\n",
    "print(f\"Accidents assigned to segments: {len(joined_nearest_unique)}\")\n",
    "print(f\"Ratio of assigned accidents: {len(joined_nearest_unique) / len(accidents_bike_berlin):.2%}\")\n",
    "\n",
    "joined_nearest_unique.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee627d80",
   "metadata": {},
   "source": [
    "## Strava data (bicycle network traffic, other features - daily)\n",
    "\n",
    "1. We need to aggregate this df to the same granularity as in Accidents data (segment, year, month, weekday) to join. \n",
    "2. We can not join only by geo data, as Accidents don't have date column, but Strava contains daily info (eg specific traffic volume or weather on specific day)\n",
    "3. We can not just calculate mean of all columns in Strava data by year-month-etc... as we also have categorial features (for example `infrastructure_bicyclelane_type`) and some features are constant over time for segment (eg `infrastructure_max_speed` in dataset is constant for segment for all dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f6151a",
   "metadata": {},
   "outputs": [],
   "source": [
    "strava_berlin_data = load_strava_berlin_data()\n",
    "strava_berlin_data.columns.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2f3c7f",
   "metadata": {},
   "source": [
    "### Which data types we have as features?\n",
    "\n",
    "Results:\n",
    "\n",
    "1. Mostly we have numerical features, but also categorical ones like `'infrastructure_bicyclelane_type'` - we will check if we need to aggregate them somehow or they are contstant over time.\n",
    "2. Analysis shows:\n",
    "    - **Numeric columns (111)**: Traffic counts, speeds, socioeconomic indicators, weather data\n",
    "    - **Categorical columns**: Infrastructure types, activity types, street properties\n",
    "    - **Boolean columns (8)**: Holiday flags, weekend indicators, data quality flags\n",
    "3. **Key finding**: All connectivity and infrastructure columns are constant per segment, so they only need to be taken once per segment. Socioeconomic, motorized, strava, and weather columns vary over time and require aggregation by year-month-weekday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aad810",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = strava_berlin_data\n",
    "numeric_cols = df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(exclude=[\"number\"]).columns.tolist()\n",
    "bool_cols = df.select_dtypes(include=[\"bool\"]).columns.tolist()\n",
    "\n",
    "print(\"Numeric:\", len(numeric_cols))\n",
    "print(numeric_cols)\n",
    "print(\"\\nCategorical:\", len(categorical_cols))\n",
    "print(categorical_cols)\n",
    "print(\"\\nBool:\", len(bool_cols))\n",
    "print(bool_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141ff808",
   "metadata": {},
   "source": [
    "### Check which features we have contstant for one segment over time, so we don't need to aggregate them futher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81144da",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = column_stability_summary(strava_berlin_data, group_col=\"counter_name\")\n",
    "summary_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097f6af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag dtypes\n",
    "col_dtype = strava_berlin_data.dtypes\n",
    "summary_df[\"dtype_bucket\"] = summary_df[\"column\"].map(\n",
    "    lambda c: \"bool\" if col_dtype[c].name == \"bool\"\n",
    "    else \"numeric\" if np.issubdtype(col_dtype[c], np.number)\n",
    "    else \"categorical\"\n",
    ")\n",
    "\n",
    "# overall constant/varying summary\n",
    "overall_stats = {\n",
    "    \"total_columns\": len(summary_df),\n",
    "    \"constant_columns\": int((summary_df[\"segments_varying\"] == 0).sum()),\n",
    "    \"varying_columns\": int((summary_df[\"segments_varying\"] > 0).sum()),\n",
    "}\n",
    "overall_stats[\"percent_constant\"] = round(\n",
    "    overall_stats[\"constant_columns\"]\n",
    "    / max(overall_stats[\"total_columns\"], 1)\n",
    "    * 100,\n",
    "    1,\n",
    " )\n",
    "\n",
    "print(\"Overall column stability:\")\n",
    "for key, value in overall_stats.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# dtype-level statistics\n",
    "dtype_counts = (\n",
    "    summary_df\n",
    "    .groupby([\"dtype_bucket\"])\n",
    "    .agg(\n",
    "        total_cols=(\"column\", \"count\"),\n",
    "        constant_cols=(\"segments_varying\", lambda s: (s == 0).sum()),\n",
    "        varying_cols=(\"segments_varying\", lambda s: (s > 0).sum()),\n",
    "    )\n",
    ")\n",
    "\n",
    "dtype_counts[\"percent_constant\"] = (\n",
    "    dtype_counts[\"constant_cols\"] / dtype_counts[\"total_cols\"] * 100\n",
    ").round(1)\n",
    "\n",
    "display(dtype_counts.sort_values(\"percent_constant\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a115515d",
   "metadata": {},
   "source": [
    "### Result: All connectivity and infrastructure columns are constant per segment. Socioeconomic, Motorized and weather columns vary, so we need to aggregate them.\n",
    "\n",
    "- Connectivity (7/7 constant, 2 bool, 5 numeric): treat as static attributes per segment; just carry a single value (e.g., first).\n",
    "- Infrastructure (58/58 constant, 1 bool, 10 categorical, 47 numeric): fully static; keep one value per segment, no temporal aggregation needed.\n",
    "**- Other (14 cols, 5 constant/9 varying; 5 bool/6 cat/3 num): mixed bag—decide column by column; reassign misfiled cols if any.**\n",
    "- Motorized (12/12 varying, all numeric): fully time-varying; aggregate over your time buckets (sum for counts, mean for speeds).\n",
    "- Socioeconomic (17/17 varying, numeric): varies across time in the data; aggregate over your time buckets (sum for counts, mean for speeds).\n",
    "**- Strava (19/19 varying; 1 categorical, 18 numeric): counts/speeds should be summed/averaged per time bucket; handle the single categorical (strava_activity_type) via ????**\n",
    "- Weather (9/9 varying, numeric): time-varying; aggregate with mean (or min/max if useful).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249b7669",
   "metadata": {},
   "source": [
    "## Aggregation of Berlin Strava data \n",
    "1. Aggregation keys: counter_name (segment), year, month, weekday (to align with accidents).\n",
    "2. Constant features stay as-is (no aggregation) since they don’t vary over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45fcb60",
   "metadata": {},
   "source": [
    "### This code execution can take a while, on Liaisan's pc ~13 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33176aa4",
   "metadata": {},
   "source": [
    "# Segment level risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1a9a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Build exposure panel from Strava/sensor data (segment x year x month) ---\n",
    "final_exposure_ym = build_exposure_panel_segment_year_month(\n",
    "    strava_berlin_data,\n",
    "    segment_static=segment_static,\n",
    "    summary_df=summary_df,\n",
    ")\n",
    "\n",
    "print(\"Exposure panel (segment–year–month) shape:\", final_exposure_ym.shape)\n",
    "final_exposure_ym.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41645bfd",
   "metadata": {},
   "source": [
    "## Aggregate accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2a334c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Build rich accident panel from Unfallatlas+segments (segment x year x month) ---\n",
    "min_year = int(final_exposure_ym[\"year\"].min())\n",
    "max_year = int(final_exposure_ym[\"year\"].max())\n",
    "\n",
    "accidents_agg_ym_rich = aggregate_accidents_segment_year_month_rich(\n",
    "    joined_nearest_unique,\n",
    "    column_map=accident_columns_en,\n",
    "    exposure_year_min=min_year,\n",
    "    exposure_year_max=max_year,\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Rich accident aggregate (segment–year–month) shape:\",\n",
    "    accidents_agg_ym_rich.shape,\n",
    ")\n",
    "accidents_agg_ym_rich.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827b588c",
   "metadata": {},
   "source": [
    "## Merge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2df095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Merge exposure and accident panels into a risk panel ---\n",
    "\n",
    "merge_keys = [\"counter_name\", \"year\", \"month\"]\n",
    "\n",
    "merged_accidents_strava_ym = merge_exposure_and_accidents(\n",
    "    final_exposure_ym,\n",
    "    accidents_agg_ym_rich,\n",
    "    merge_keys=merge_keys,\n",
    " )\n",
    "\n",
    "print(\n",
    "    \"Merged risk panel (segment–year–month) shape:\",\n",
    "    merged_accidents_strava_ym.shape,\n",
    " )\n",
    "\n",
    "merged_accidents_strava_ym.head()\n",
    "\n",
    "# Save geodataframe to parquet file\n",
    "out_dir = _project_root / \"data\" / \"merged\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "out_path = out_dir / \"berlin_bike_accident_strava_panel.parquet\"\n",
    "\n",
    "gpd.GeoDataFrame(\n",
    "    merged_accidents_strava_ym,\n",
    "    geometry=\"geometry\",\n",
    "    crs=segment_geo_gdf.crs,\n",
    " ).to_parquet(\n",
    "    out_path,\n",
    "    index=False,\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32099b10",
   "metadata": {},
   "source": [
    "## Sanity check of the merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb023c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = sanity_check_merge(\n",
    "    merged_accidents_strava_ym=merged_accidents_strava_ym,\n",
    "    accidents_agg_ym_rich=accidents_agg_ym_rich,\n",
    "    final_exposure_ym=final_exposure_ym,\n",
    ")\n",
    "\n",
    "for k, v in stats.items():\n",
    "    print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57547572",
   "metadata": {},
   "source": [
    "# Playground, to create and test smaller dataset version. The smaller ones are created based on the completly merged version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06828c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "core_panel = build_core_risk_panel(merged_accidents_strava_ym)\n",
    "print(\"Core panel shape:\", core_panel.shape)\n",
    "core_panel.head(20)\n",
    "\n",
    "# Save geodataframe to parquet file\n",
    "out_dir = _project_root / \"data\" / \"merged\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "out_path = out_dir / \"berlin_bike_accident_strava_risk_core_panel.parquet\"\n",
    "\n",
    "gpd.GeoDataFrame(\n",
    "    core_panel,\n",
    "    geometry=\"geometry\",\n",
    "    crs=segment_geo_gdf.crs,\n",
    " ).to_parquet(\n",
    "    out_path,\n",
    "    index=False,\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d49be60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find unique numbers of accidents in core_panel\n",
    "unique_accident_counts = core_panel[\"total_accidents\"].nunique()\n",
    "print(f\"Unique accident counts in core panel: {unique_accident_counts}\")\n",
    "\n",
    "# display those unique counts\n",
    "print(\"Unique accident counts:\", core_panel[\"total_accidents\"].unique())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2630a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum up the accidents per segment to verfiy total accidents per segment\n",
    "accidents_per_segment = (\n",
    "    core_panel\n",
    "    .groupby(\"counter_name\", as_index=False)[\"total_accidents\"]\n",
    "    .sum()\n",
    "    .rename(columns={\"total_accidents\": \"total_accidents_segment\"})\n",
    ")\n",
    "\n",
    "# find segments with highest total accidents\n",
    "top_segments = accidents_per_segment.sort_values(\"total_accidents_segment\", ascending=False).head(10)\n",
    "print(\"Top 10 segments by total accidents:\")\n",
    "display(top_segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6979476c",
   "metadata": {},
   "source": [
    "# Crossing (junction) risk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfd8bdf",
   "metadata": {},
   "source": [
    "### Build nodes (junction candidates) from segment endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edb6e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_raw = build_nodes_from_segment_endpoints(\n",
    "    segment_geo_gdf,\n",
    "    counter_col=\"counter_name\",\n",
    ")\n",
    "\n",
    "nodes_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17d275d",
   "metadata": {},
   "source": [
    "### Cluster endpoints into nodes (snap grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fe3a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = cluster_nodes_snap_grid(\n",
    "    nodes_raw,\n",
    "    tol_m=2,\n",
    "    counter_col=\"counter_name\",\n",
    ")\n",
    "\n",
    "nodes_raw = clustering.nodes_raw\n",
    "node_points = clustering.node_points\n",
    "segment_node_map = clustering.segment_node_map\n",
    "\n",
    "print(\"Nodes (raw endpoints):\", len(nodes_raw))\n",
    "print(\"Nodes (clustered):\", len(node_points))\n",
    "segment_node_map.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92220f0",
   "metadata": {},
   "source": [
    "### Define crossings (nodes with degree $\\geq$ 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a4d6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossing_ids = select_crossings_by_degree(\n",
    "    nodes_raw,\n",
    "    min_degree=3,\n",
    "    counter_col=\"counter_name\",\n",
    ")\n",
    "\n",
    "crossings_gdf = node_points[node_points[\"node_id\"].isin(crossing_ids)].copy()\n",
    "\n",
    "print(\"Crossings (degree >= 3):\", len(crossings_gdf))\n",
    "crossings_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba02b7f",
   "metadata": {},
   "source": [
    "### Assign accidents to nearest crossing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626622c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_node, acc_node_ym = assign_accidents_to_nearest_crossing(\n",
    "    joined_nearest_unique,\n",
    "    crossings_gdf,\n",
    "    max_distance_m=20,\n",
    ")\n",
    "\n",
    "print(\"Accidents assigned to crossings:\", len(acc_node))\n",
    "print(\"Accident groups (node×year×month):\", len(acc_node_ym))\n",
    "acc_node_ym.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57c0604",
   "metadata": {},
   "source": [
    "### Build node-level exposure from segment flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a43f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_exposure_ym = build_node_exposure_panel_from_segments(\n",
    "    final_exposure_ym,\n",
    "    segment_node_map,\n",
    "    crossing_ids,\n",
    "    trip_col=\"sum_strava_total_trip_count\",\n",
    ")\n",
    "\n",
    "print(\"Node exposure (node×year×month) shape:\", node_exposure_ym.shape)\n",
    "node_exposure_ym.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704ec732",
   "metadata": {},
   "source": [
    "### Combine into node-level risk panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222579c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_panel_ym = build_node_risk_panel(\n",
    "    node_exposure_ym,\n",
    "    acc_node_ym,\n",
    "    crossings_gdf,\n",
    ")\n",
    "\n",
    "print(\"Node panel (crossing x year x month) shape:\", node_panel_ym.shape)\n",
    "node_panel_ym.head()\n",
    "\n",
    "# Save geodataframe to parquet file\n",
    "out_dir = _project_root / \"data\" / \"merged\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "out_path = out_dir / \"berlin_bike_accident_node_panel.parquet\"\n",
    "\n",
    "node_panel_ym.to_parquet(\n",
    "    out_path,\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b0b6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the overall description of the dataset\n",
    "node_panel_ym.describe(include=\"all\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
